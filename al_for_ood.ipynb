{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshitt/AL-OOD/blob/main/al_for_ood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slqlFCF3Clb1"
      },
      "source": [
        "#Deleting files in the Trash that is occupying space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ydWrCdBCkRx"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "my_drive = GoogleDrive(gauth)\n",
        "\n",
        "for a_file in my_drive.ListFile({'q': \"trashed = true\"}).GetList():\n",
        "    # print the name of the file being deleted.\n",
        "    print({a_file['title']},\"is about to get deleted permanently.\")\n",
        "    # delete the file permanently.\n",
        "    a_file.Delete()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wrxwQi_DuOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9098919c-447c-453a-c83a-6d50ea9d3fb3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4smBOmU5H7Rh",
        "outputId": "e30e7632-b7fd-45b2-9e83-3ec5060b67e1"
      },
      "source": [
        "#!du -h --max-depth 3 /content/drive/Shareddrives\n",
        "!du -hs /content/drive/Shareddrives\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du: cannot read directory '/content/drive/Shareddrives/Deep_Active_Learning/camelyon17_v1.0/patches/patient_096_node_0': Input/output error\n",
            "du: cannot read directory '/content/drive/Shareddrives/Deep_Active_Learning/nihdataset1': Input/output error\n",
            "53G\t/content/drive/Shareddrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqXhnco6-MmV",
        "outputId": "9b8395db-0f56-4b8e-a348-220671f1f262"
      },
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/al4ood/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu1d3BydtzUR",
        "outputId": "e3c4afa0-eaea-45be-c16b-c05b28d0a04a"
      },
      "source": [
        "!ls /content/drive/Shareddrives/Deep_Active_Learning/nihdataset1 | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "r4egr9WS-cWP",
        "outputId": "6c2b685c-76c7-499f-fdd0-65671732219f"
      },
      "source": [
        "#!cd /content/drive/Shareddrives/Deep_Active_Learning/nihfromkaggle\n",
        "#!unzip \"/content/drive/Shareddrives/Deep_Active_Learning/*.zip\"\n",
        "#from zipfile import ZipFile\n",
        "#!rm -r /content/drive/Shareddrives/Deep_Active_Learning/nihfromkaggle\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "my_dir = r\"/content/drive/Shareddrives/Deep_Active_Learning/nihdataset1\"\n",
        "my_zip = r\"/content/drive/Shareddrives/Deep_Active_Learning/data.zip\"\n",
        "\n",
        "with zipfile.ZipFile(my_zip) as zip_file:\n",
        "    for member in zip_file.namelist():\n",
        "        filename = os.path.basename(member)\n",
        "        # skip directories\n",
        "        if not filename:\n",
        "            continue\n",
        "\n",
        "        # copy file (taken from zipfile's extract)\n",
        "        source = zip_file.open(member)\n",
        "\n",
        "        if not Path(os.path.join(my_dir, filename)).is_file():\n",
        "          target = open(os.path.join(my_dir, filename), \"wb\")\n",
        "          with source, target:\n",
        "            shutil.copyfileobj(source, target)\n",
        "          target.close()\n",
        "\n",
        "        source.close()\n",
        "\n",
        "#!du -h --max-depth=2 /content/drive/Shareddrives/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-42b32f7a853b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# copy file (taken from zipfile's extract)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[0mfheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructFileHeader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_SIGNATURE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstringFileHeader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bad magic number for file header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzef_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_FILENAME_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: Bad magic number for file header"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-11-25T17:45:31.038861Z",
          "iopub.execute_input": "2021-11-25T17:45:31.039687Z",
          "iopub.status.idle": "2021-11-25T17:45:33.546241Z",
          "shell.execute_reply.started": "2021-11-25T17:45:31.039573Z",
          "shell.execute_reply": "2021-11-25T17:45:33.545281Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isNnij1FqpmC",
        "outputId": "c6a93a1d-9796-4f1b-b0fa-4272cf763120"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as trns\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import h5py\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import copy\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import PIL.Image as Image\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from matplotlib import pyplot as plt\n",
        "from distil.distil.utils.models.resnet import ResNet18\n",
        "from trust.trust.utils.custom_dataset import load_dataset_custom\n",
        "from torch.utils.data import Subset\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "from math import floor\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from distil.distil.active_learning_strategies.scmi import SCMI\n",
        "from distil.distil.active_learning_strategies.smi import SMI\n",
        "from distil.distil.active_learning_strategies.badge import BADGE\n",
        "from distil.distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
        "from distil.distil.active_learning_strategies.gradmatch_active import GradMatchActive\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "from distil.distil.utils.utils import *\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMHvm-pqpmP"
      },
      "source": [
        "# Distil related libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-24T09:06:40.07148Z",
          "iopub.execute_input": "2021-11-24T09:06:40.072071Z",
          "iopub.status.idle": "2021-11-24T09:11:22.084239Z",
          "shell.execute_reply.started": "2021-11-24T09:06:40.07203Z",
          "shell.execute_reply": "2021-11-24T09:11:22.083181Z"
        },
        "trusted": true,
        "id": "iMaaVkSZqpmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2038e00f-3a95-469c-f895-06815c939cd3"
      },
      "source": [
        "!pip3 install sphinxcontrib-napoleon\n",
        "!pip3 install sphinxcontrib-bibtex\n",
        "!pip3 install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ submodlib\n",
        "%cd /content/drive/Shareddrives/Deep_Active_Learning/\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sphinxcontrib-napoleon in /usr/local/lib/python3.7/dist-packages (0.7)\n",
            "Requirement already satisfied: pockets>=0.3 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-napoleon) (0.9.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-napoleon) (1.15.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3105, in __init__\n",
            "    project_name = safe_name(self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1322, in safe_name\n",
            "    return re.sub('[^A-Za-z0-9.]+', '-', name)\n",
            "  File \"/usr/lib/python3.7/re.py\", line 194, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "Requirement already satisfied: sphinxcontrib-bibtex in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: docutils>=0.8 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex) (0.17.1)\n",
            "Requirement already satisfied: Sphinx>=2.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex) (4.3.1)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex) (1.0.1)\n",
            "Requirement already satisfied: pybtex>=0.20 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex) (0.24.0)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from pybtex>=0.20->sphinxcontrib-bibtex) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pybtex>=0.20->sphinxcontrib-bibtex) (1.15.0)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.7/dist-packages (from pybtex>=0.20->sphinxcontrib-bibtex) (3.13)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.11.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (57.4.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.0.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.3.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (0.7.12)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.9.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (21.3)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex) (1.0.2)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->Sphinx>=2.1->sphinxcontrib-bibtex) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->Sphinx>=2.1->sphinxcontrib-bibtex) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex) (2021.10.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->Sphinx>=2.1->sphinxcontrib-bibtex) (3.0.6)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple/\n",
            "Requirement already satisfied: submodlib in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from submodlib) (1.19.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from submodlib) (0.51.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from submodlib) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from submodlib) (0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->submodlib) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->submodlib) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->submodlib) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->submodlib) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->submodlib) (1.1.0)\n",
            "/content/drive/Shareddrives/Deep_Active_Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-24T09:16:20.951852Z",
          "iopub.execute_input": "2021-11-24T09:16:20.952132Z",
          "iopub.status.idle": "2021-11-24T09:16:20.956261Z",
          "shell.execute_reply.started": "2021-11-24T09:16:20.952081Z",
          "shell.execute_reply": "2021-11-24T09:16:20.955504Z"
        },
        "trusted": true,
        "id": "xazTRiCCqpmU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "cf413820-7fc6-45c8-ad2e-dbdc67c2cd64"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/Deep_Active_Learning/distil')\n",
        "    from distil.utils.models.resnet import ResNet18"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5817a0b94b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shareddrives/Deep_Active_Learning/distil'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdistil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distil'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizvhjm0qpmW"
      },
      "source": [
        "# Map images to their location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfFxsYJqpmZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T17:47:01.743691Z",
          "iopub.execute_input": "2021-11-25T17:47:01.744016Z",
          "iopub.status.idle": "2021-11-25T17:47:41.886941Z",
          "shell.execute_reply.started": "2021-11-25T17:47:01.743972Z",
          "shell.execute_reply": "2021-11-25T17:47:41.886018Z"
        },
        "trusted": true,
        "id": "oRE1Ndw1qpma"
      },
      "source": [
        "AllClasses = {\n",
        "    'Atelectasis':0,\n",
        "    'Consolidation':1,\n",
        "    'Infiltration':2,\n",
        "    'Edema':3,\n",
        "    'Emphysema':4,\n",
        "    'Fibrosis':5,\n",
        "    'Effusion':6,\n",
        "    'Pneumonia':7,\n",
        "    'Pleural_Thickening':8,\n",
        "    'Hernia':9,\n",
        "    'Cardiomegaly':10,\n",
        "    'Nodule':11,\n",
        "    'Mass':12,\n",
        "    'Pneumothorax':13,\n",
        "    'No Finding':14\n",
        "}\n",
        "\n",
        "InClasses = {\n",
        "    'Atelectasis':0,\n",
        "    'Consolidation':1,\n",
        "    'Infiltration':2,\n",
        "    'Edema':3,\n",
        "    'Emphysema':4,\n",
        "    'Fibrosis':5,\n",
        "    'Effusion':6,\n",
        "    'Pneumonia':7,\n",
        "    'Pleural_Thickening':8,\n",
        "    'Hernia':9\n",
        "}\n",
        "\n",
        "OutClasses={\n",
        "    'Cardiomegaly':10,\n",
        "    'Nodule':11,\n",
        "    'Mass':12,\n",
        "    'Pneumothorax':13\n",
        "}\n",
        "\n",
        "path = '/content/drive/Shareddrives/Deep_Active_Learning/chestxray/'\n",
        "\n",
        "train = open(f'{path}train_val_list.txt')\n",
        "test = open(f'{path}test_list.txt')\n",
        "train_img = train.read().split()\n",
        "test_img = test.read().split()\n",
        "train.close()\n",
        "test.close()\n",
        "\n",
        "# for i in range(12):\n",
        "#     path = f'{path}images_' + ('00' if i < 9 else '0') + str(i + 1) + '/images/'\n",
        "#     print(path)\n",
        "#     for f in os.listdir(path):\n",
        "#         if os.path.isfile(os.path.join(path, f)) and f in train_img:\n",
        "#             os.rename(os.path.join(path, f), os.path.join(\"/content/drive/Shareddrives/Deep_Active_Learning/chestxray/all_images/train\",f))\n",
        "#         if os.path.isfile(os.path.join(path, f)) and f in test_img:\n",
        "#             os.rename(os.path.join(path, f), os.path.join(\"/content/drive/Shareddrives/Deep_Active_Learning/chestxray/all_images/test\",f))\n",
        "\n",
        "df = pd.read_csv(f'{path}Data_Entry_2017.csv')\n",
        "df.head()\n",
        "\n",
        "num_train = 100\n",
        "num_test = 1000\n",
        "\n",
        "train = []\n",
        "for i in train_img[-1*num_train:]:\n",
        "  #print(i)\n",
        "  label = df.loc[df['Image Index'] == i,'Finding Labels'].values[0]\n",
        "  if len(label.split('|'))==1:\n",
        "      train.append([i,AllClasses[label]])\n",
        "  else:\n",
        "      continue\n",
        "test = []\n",
        "for i in test_img[-1*num_test:]:\n",
        "  label = df.loc[df['Image Index'] == i,'Finding Labels'].values[0]\n",
        "  if len(label.split('|'))==1:\n",
        "      test.append([i,AllClasses[label]])\n",
        "  else:\n",
        "      continue\n",
        "\n",
        "fullset=pd.DataFrame(train, columns = ['Image', 'Label'])\n",
        "df_train = pd.DataFrame(train, columns = ['Image', 'Label'])\n",
        "df_test = pd.DataFrame(test, columns = ['Image', 'Label'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pZNvEn6qpme"
      },
      "source": [
        "class ChestXray(Dataset):\n",
        "    def __init__(self, data, root='/content/drive/Shareddrives/Deep_Active_Learning/chestxray/all_images', transform=None):\n",
        "        self.root = root\n",
        "        self.image_paths = [os.path.join(root,s) for s in data['Image']]\n",
        "\n",
        "        self.targets = data['Label'].tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAfRDE-lqpmg"
      },
      "source": [
        "xray_dataset = ChestXray(data=df_test)\n",
        "print(xray_dataset[1][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMfvhh6Kmqbz"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQUIHP_5_vT8"
      },
      "source": [
        "def create_ood_data(fullset, testset, split_cfg, num_cls, augVal):\n",
        "\n",
        "    np.random.seed(42)\n",
        "    train_idx = []\n",
        "    val_idx = []\n",
        "    lake_idx = []\n",
        "    test_idx = []\n",
        "    selected_classes = np.array(list(range(split_cfg['num_cls_idc'])))\n",
        "    for i in range(num_cls): #all_classes\n",
        "        full_idx_class = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())\n",
        "        if(i in selected_classes):\n",
        "            test_idx_class = list(torch.where(torch.Tensor(testset.targets) == i)[0].cpu().numpy())\n",
        "            test_idx += test_idx_class\n",
        "            class_train_idx = list(np.random.choice(np.array(full_idx_class), size=split_cfg['per_idc_train'], replace=False))\n",
        "            train_idx += class_train_idx\n",
        "            remain_idx = list(set(full_idx_class) - set(class_train_idx))\n",
        "            class_val_idx = list(np.random.choice(np.array(remain_idx), size=split_cfg['per_idc_val'], replace=False))\n",
        "            remain_idx = list(set(remain_idx) - set(class_val_idx))\n",
        "            class_lake_idx = list(np.random.choice(np.array(remain_idx), size=split_cfg['per_idc_lake'], replace=False))\n",
        "        else:\n",
        "            class_train_idx = list(np.random.choice(np.array(full_idx_class), size=split_cfg['per_ood_train'], replace=False)) #always 0\n",
        "            remain_idx = list(set(full_idx_class) - set(class_train_idx))\n",
        "            class_val_idx = list(np.random.choice(np.array(remain_idx), size=split_cfg['per_ood_val'], replace=False)) #Only for CG ood val has samples\n",
        "            remain_idx = list(set(remain_idx) - set(class_val_idx))\n",
        "            class_lake_idx = list(np.random.choice(np.array(remain_idx), size=split_cfg['per_ood_lake'], replace=False)) #many ood samples in lake\n",
        "\n",
        "        if(augVal and (i in selected_classes)): #augment with samples only from the imbalanced classes\n",
        "            train_idx += class_val_idx\n",
        "        val_idx += class_val_idx\n",
        "        lake_idx += class_lake_idx\n",
        "\n",
        "    train_set = SubsetWithTargets(fullset, train_idx, torch.Tensor(fullset.targets)[train_idx])\n",
        "    val_set = SubsetWithTargets(fullset, val_idx, torch.Tensor(fullset.targets)[val_idx])\n",
        "    lake_set = SubsetWithTargets(fullset, lake_idx, getOODtargets(torch.Tensor(fullset.targets)[lake_idx], selected_classes, split_cfg['num_cls_idc']))\n",
        "    test_set = SubsetWithTargets(testset, test_idx, torch.Tensor(testset.targets)[test_idx])\n",
        "\n",
        "    return train_set, val_set, test_set, lake_set, selected_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQcZRCLxwm9"
      },
      "source": [
        "split_cfg = {\n",
        "    'per_imbclass_train': int(),\n",
        "    'per_imbclass_val': int(),\n",
        "    'per_imbclass_lake': int(),\n",
        "    'per_class_train': int(1),\n",
        "    'per_class_val': int(1),\n",
        "    'per_class_lake': int(5),\n",
        "    'sel_cls_idx': list(),\n",
        "    'train_size': int(),\n",
        "    'val_size': int(),\n",
        "    'lake_size': int(),\n",
        "    'num_rep': int(),\n",
        "    'lake_subset_repeat_size': int(),\n",
        "    'num_cls_imbalance': int(),\n",
        "    'num_cls_idc': int(10),\n",
        "    'per_idc_train': int(1),\n",
        "    'per_idc_val': int(1),\n",
        "    'per_idc_lake': int(5),\n",
        "    'per_ood_train': int(1),\n",
        "    'per_ood_val': int(1),\n",
        "    'per_ood_lake': int(5)\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLPdp3RSqRpA"
      },
      "source": [
        "create_ood_data(df_train, df_test, split_cfg, num_cls=10, augVal=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzE6MzJllUAz"
      },
      "source": [
        "## SMI AL Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "babbetEflZer"
      },
      "source": [
        "def model_eval_loss(data_loader, model, criterion):\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss\n",
        "\n",
        "def init_weights(m):\n",
        "#     torch.manual_seed(35)\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "def create_model(name, num_cls, device, embedding_type):\n",
        "    if name == 'ResNet18':\n",
        "        if embedding_type == \"gradients\":\n",
        "            model = ResNet18(num_cls)\n",
        "        else:\n",
        "            model = models.resnet18()\n",
        "    elif name == 'MnistNet':\n",
        "        model = MnistNet()\n",
        "    elif name == 'ResNet164':\n",
        "        model = ResNet164(num_cls)\n",
        "    model.apply(init_weights)\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "def loss_function():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
        "    return criterion, criterion_nored\n",
        "\n",
        "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                          momentum=m, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
        "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                          momentum=m, weight_decay=wd)\n",
        "    return optimizer\n",
        "\n",
        "def generate_cumulative_timing(mod_timing):\n",
        "    tmp = 0\n",
        "    mod_cum_timing = np.zeros(len(mod_timing))\n",
        "    for i in range(len(mod_timing)):\n",
        "        tmp += mod_timing[i]\n",
        "        mod_cum_timing[i] = tmp\n",
        "    return mod_cum_timing/3600\n",
        "\n",
        "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications,\n",
        "                       final_tst_predictions, saveDir, prefix):\n",
        "    #find queries from the validation set that are erroneous\n",
        "#     saveDir = os.path.join(saveDir, prefix)\n",
        "#     if(not(os.path.exists(saveDir))):\n",
        "#         os.mkdir(saveDir)\n",
        "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
        "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
        "    val_class_err_idxs = []\n",
        "    tst_err_log = []\n",
        "    val_err_log = []\n",
        "    for i in range(num_cls):\n",
        "        if(feature==\"ood\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets.float()) == i)[0].cpu().numpy())\n",
        "        if(feature==\"classimb\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
        "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
        "        #err classifications per class\n",
        "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
        "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
        "        if(len(val_class_idxs)>0):\n",
        "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
        "        else:\n",
        "            val_error_perc = 0\n",
        "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
        "        print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
        "        val_class_err_idxs.append(val_err_class_idx)\n",
        "        tst_err_log.append(tst_error_perc)\n",
        "        val_err_log.append(val_error_perc)\n",
        "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
        "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
        "    return tst_err_log, val_err_log, val_class_err_idxs\n",
        "\n",
        "\n",
        "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
        "    all_lake_idx = list(range(len(lake_set)))\n",
        "    if(not(len(subset)==budget) and augrandom):\n",
        "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
        "        remain_budget = int(budget) - len(subset)\n",
        "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
        "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
        "        subset += random_subset_idx\n",
        "    lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
        "    if(feature==\"ood\"):\n",
        "        ood_lake_idx = list(set(lake_subset_idxs)-set(subset))\n",
        "        private_set =  SubsetWithTargets(true_lake_set, ood_lake_idx, torch.Tensor(np.array([split_cfg['num_cls_idc']]*len(ood_lake_idx))).float())\n",
        "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
        "    remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
        "    remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
        "    print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
        "    if(feature!=\"ood\"): assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
        "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
        "    if(feature==\"ood\"):\n",
        "        return aug_train_set, remain_lake_set, remain_true_lake_set, private_set, lake_ss\n",
        "    else:\n",
        "        return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
        "\n",
        "def getQuerySet(val_set, val_class_err_idxs, imb_cls_idx, miscls):\n",
        "    miscls_idx = []\n",
        "    if(miscls):\n",
        "        for i in range(len(val_class_err_idxs)):\n",
        "            if i in imb_cls_idx:\n",
        "                miscls_idx += val_class_err_idxs[i]\n",
        "        print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
        "    else:\n",
        "        for i in imb_cls_idx:\n",
        "            imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
        "            miscls_idx += imb_cls_samples\n",
        "        print(\"total samples from imb classes as targets: \", len(miscls_idx))\n",
        "    return Subset(val_set, miscls_idx)\n",
        "\n",
        "\n",
        "def getPrivateSet(lake_set, subset, private_set):\n",
        "    #augment prev private set and current subset\n",
        "    new_private_set = SubsetWithTargets(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
        "#     new_private_set =  Subset(lake_set, subset)\n",
        "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
        "    return total_private_set\n",
        "\n",
        "def remove_ood_points(lake_set, subset, idc_idx):\n",
        "    idx_subset = []\n",
        "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
        "    for i in idc_idx:\n",
        "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
        "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
        "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
        "    return idx_subset\n",
        "\n",
        "def getPerClassSel(lake_set, subset, num_cls):\n",
        "    perClsSel = []\n",
        "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
        "    for i in range(num_cls):\n",
        "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
        "        perClsSel.append(len(cls_subset_idx))\n",
        "    return perClsSel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLmqRuEtxZsG"
      },
      "source": [
        "feature = \"ood\"\n",
        "device_id = 0\n",
        "run=\"fkna_3\"\n",
        "datadir = 'data/'\n",
        "data_name = 'ChestXray'\n",
        "model_name = 'ResNet18'\n",
        "num_rep = 10\n",
        "learning_rate = 0.01\n",
        "num_runs = 1  # number of random runs\n",
        "computeClassErrorLog = True\n",
        "\n",
        "magnification = 1\n",
        "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
        "datkbuildPath = \"./datk/build\"\n",
        "exePath = \"cifarSubsetSelector\"\n",
        "print(\"Using Device:\", device)\n",
        "doublePrecision = True\n",
        "linearLayer = True\n",
        "miscls = False\n",
        "# handler = DataHandler_CIFAR10\n",
        "augTarget = True\n",
        "embedding_type = \"gradients\"\n",
        "\n",
        "if(feature==\"ood\"):\n",
        "    num_cls=8\n",
        "    budget=250\n",
        "    num_epochs = int(10)\n",
        "    split_cfg = {'num_cls_idc':8, 'per_idc_train':200, 'per_idc_val':10, 'per_idc_lake':500, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':5000}#cifar10\n",
        "    # split_cfg = {'num_cls_idc':50, 'per_idc_train':100, 'per_idc_val':2, 'per_idc_lake':100, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':500}#cifar100\n",
        "    initModelPath = \"weights/\" + data_name + \"_\" + feature + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_idc_train\"]) + \"_\" + str(split_cfg[\"per_idc_val\"]) + \"_\" + str(split_cfg[\"num_cls_idc\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}