{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a78a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data2/akshit/Pneumonia\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdcf2e9-2554-409a-8a5d-74041565ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import cifar\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append('/mnt/data2/akshit/distil/')\n",
    "sys.path.append('/mnt/data2/akshit/trust/')\n",
    "from distil.utils.models.resnet import ResNet18\n",
    "from trust.utils.pneumoniamnist import load_dataset_custom_1\n",
    "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from distil.active_learning_strategies.scmi import SCMI\n",
    "from distil.active_learning_strategies.smi import SMI\n",
    "from distil.active_learning_strategies.badge import BADGE\n",
    "from distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
    "from distil.active_learning_strategies.gradmatch_active import GradMatchActive\n",
    "from distil.active_learning_strategies.glister import GLISTER\n",
    "\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from distil.utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515adf30-9e7e-4289-a788-c1b8fc473c4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29c1020-88fb-4d06-b982-ca55cc662ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "#     saveDir = os.path.join(saveDir, prefix)\n",
    "#     if(not(os.path.exists(saveDir))):\n",
    "#         os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        if(feature==\"ood\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        if(feature==\"classimb\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "            \n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "        print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    if(feature==\"ood\"): \n",
    "        ood_lake_idx = list(set(lake_subset_idxs)-set(subset))\n",
    "        private_set =  SubsetWithTargets(true_lake_set, ood_lake_idx, torch.Tensor(np.array([split_cfg['num_cls_idc']]*len(ood_lake_idx))).float())\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "    print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    if(feature!=\"ood\"): assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    if(feature==\"ood\"): \n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, private_set, lake_ss\n",
    "    else:\n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set, val_class_err_idxs, imb_cls_idx, miscls):\n",
    "    miscls_idx = []\n",
    "    if(miscls):\n",
    "        for i in range(len(val_class_err_idxs)):\n",
    "            if i in imb_cls_idx:\n",
    "                miscls_idx += val_class_err_idxs[i]\n",
    "        print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    else:\n",
    "        for i in imb_cls_idx:\n",
    "            imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "            miscls_idx += imb_cls_samples\n",
    "        print(\"total samples from imb classes as targets: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = SubsetWithTargets(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b63cd6-d60a-4471-a37f-31758df42dad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e861df-0986-4ba3-8c1b-1c770732b526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "feature = \"ood\"\n",
    "device_id = 0\n",
    "run=\"fkna_3\"\n",
    "datadir = 'data/'\n",
    "data_name = 'cifar10'\n",
    "model_name = 'ResNet18'\n",
    "num_rep = 10\n",
    "learning_rate = 0.01\n",
    "num_runs = 1  # number of random runs\n",
    "computeClassErrorLog = True\n",
    "\n",
    "magnification = 1\n",
    "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"./datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = True\n",
    "linearLayer = True\n",
    "miscls = False\n",
    "# handler = DataHandler_CIFAR10\n",
    "augTarget = True\n",
    "embedding_type = \"gradients\"\n",
    "\n",
    "if(feature==\"ood\"):\n",
    "    num_cls=2\n",
    "    budget=20\n",
    "    num_epochs = int(10)\n",
    "    split_cfg = {'num_cls_idc':2, 'per_idc_train':5, 'per_idc_val':10, 'per_idc_lake':2500, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':5000}\n",
    "    # split_cfg = {'num_cls_idc':50, 'per_idc_train':100, 'per_idc_val':2, 'per_idc_lake':100, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':500}#cifar100\n",
    "    initModelPath = \"/mnt/data2/akshit/Pneumonia/weights/\" + data_name + \"_\" + feature + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_idc_train\"]) + \"_\" + str(split_cfg[\"per_idc_val\"]) + \"_\" + str(split_cfg[\"num_cls_idc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879579e1-6f63-4f66-b586-21b561a39f25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### AL like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa49956a-08cf-4ec0-be01-b854d9cae3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, feature, split_cfg, False, True)\n",
    "    print(\"selected classes are: \", sel_cls_idx)\n",
    "\n",
    "    if(feature==\"ood\"): num_cls+=1 #Add one class for OOD class\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "#     all_logs_dir = '/content/drive/MyDrive/research/tdss/SMI_active_learning_results_woVal/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    all_logs_dir = './SMI_active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_linear:'  + str(linearLayer) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \n",
    "                \"feature\":feature, \n",
    "                \"sel_func\":sf,\n",
    "                \"sel_budget\":budget, \n",
    "                \"num_selections\":num_epochs, \n",
    "                \"model\":model_name, \n",
    "                \"learning_rate\":learning_rate, \n",
    "                \"setting\":split_cfg, \n",
    "                \"all_class_acc\":None, \n",
    "                \"test_acc\":[],\n",
    "                \"sel_per_cls\":[], \n",
    "                \"sel_cls_idx\":sel_cls_idx.tolist()}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device, embedding_type)\n",
    "    model1 = create_model(model_name, num_cls, device, embedding_type)\n",
    "    \n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "    \n",
    "    strategy_args = {'batch_size': 20, 'device':'cuda', 'num_partitions':1, 'wrapped_strategy_class': None, \n",
    "         'embedding_type':'gradients', 'keep_embedding':False}\n",
    "    unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "    if(strategy == \"AL\"):\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_sel = GradMatchActive(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SIM\"):\n",
    "        if(sf.endswith(\"mic\")):\n",
    "            strategy_args['scmi_function'] = sf.split(\"mic\")[0] + \"cmi\"\n",
    "            strategy_sel = SCMI(train_set, unlabeled_lake_set, val_set, val_set, model, num_cls, strategy_args)\n",
    "        if(sf.endswith(\"mi\")):\n",
    "            strategy_args['smi_function'] = sf\n",
    "            strategy_sel = SMI(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "        strategy_args['verbose'] = False\n",
    "        strategy_args['optimizer'] = \"LazyGreedy\"\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        if(feature==\"ood\"): \n",
    "                            _, predicted = outputs[...,:-1].max(1)\n",
    "                        else:\n",
    "                            _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    final_tst_predictions = []\n",
    "                    final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        if(feature==\"ood\"): \n",
    "                            _, predicted = outputs[...,:-1].max(1)\n",
    "                        else:\n",
    "                            _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                        final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "            unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "            strategy_sel.update_data(train_set, unlabeled_lake_set)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append(tst_err_log)\n",
    "                val_csvlog.append(val_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                        strategy_sel.update_queries(miscls_set)\n",
    "                elif(sf.endswith(\"mic\")): #configured for the OOD setting\n",
    "                    print(\"val set targets: \", val_set.targets)\n",
    "                    strategy_sel.update_queries(val_set) #In-dist samples are in Val \n",
    "                    ########################\n",
    "                    print('Queries Updated')\n",
    "                    ########################\n",
    "                    if(len(private_set)!=0):\n",
    "                        print(\"private set targets: \", private_set.targets)\n",
    "                        strategy_sel.update_privates(private_set)\n",
    "\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\"):\n",
    "                    miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                    strategy_sel.update_queries(miscls_set)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "                \n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            \n",
    "            strategy_sel.update_model(model)\n",
    "            ########################\n",
    "            print('Model Updated')\n",
    "            ########################\n",
    "            subset = strategy_sel.select(budget)\n",
    "#             print(\"True targets of subset: \", torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "#             hypothesized_targets = strategy_sel.predict(unlabeled_lake_set)\n",
    "#             print(\"Hypothesized targets of subset: \", hypothesized_targets)\n",
    "            print(\"#### SELECTION COMPLETE ####\")\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(true_lake_set, subset, sel_cls_idx)\n",
    "            \n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\"):\n",
    "                train_set, lake_set, true_lake_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "                if(augTarget): val_set = ConcatWithTargets(val_set, add_val_set)\n",
    "            elif(feature==\"ood\"):\n",
    "                train_set, lake_set, true_lake_set, new_private_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "                train_set = torch.utils.data.ConcatDataset([train_set, new_private_set]) #Add the OOD samples with a common OOD class\n",
    "                val_set = ConcatWithTargets(val_set, add_val_set)\n",
    "                if(len(private_set)!=0):\n",
    "                    private_set = ConcatWithTargets(private_set, new_private_set)\n",
    "                else:\n",
    "                    private_set = new_private_set\n",
    "            else:\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set), \" val set: \", len(val_set))\n",
    "    \n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "            if(augTarget):\n",
    "              valloader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, pin_memory=True)\n",
    "            model = create_model(model_name, num_cls, device, strategy_args['embedding_type'])\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<300):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader): #Compute Val accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader): #Compute test accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append(tst_err_log)\n",
    "        val_csvlog.append(val_err_log)\n",
    "        print(csvlog)\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "        with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee84ac-4a90-4403-86b6-5028e0e8193d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing different algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53f371-3dfe-4561-8089-a98f7ee3c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetmic\n",
      "num ood samples:  5000\n",
      "CIFAR-10 Custom dataset stats: Train size:  40 Val size:  20 Lake size:  10000 Test set:  624\n",
      "selected classes are:  [0 1]\n",
      "Saving results to:  ./SMI_active_learning_results/cifar10/ood/logdetmic/20/fkna_3\n",
      "cifar10_ood_SIM_2_logdetmic_budget:20_epochs:10_linear:True_runsfkna_3\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  /mnt/data2/akshit/Pneumonia/weights/cifar10_ood_ResNet18_0.01_20_10_2\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  30.0 66.24\n",
      "val, test error% for class  1  :  20.0 1.54\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "14 / 20  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2022-01-04 03:10:49.274052\n",
      "14 9980 10000\n",
      "After augmentation, size of train_set:  60  lake set:  9980  val set:  34\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.1182400407269597 1.0 1.451043963432312 0.7941176470588235 4.006196945905685 0.8285256410256411 7.113529443740845\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  0.0 19.66\n",
      "val, test error% for class  1  :  50.0 15.64\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "13 / 20  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2022-01-04 03:11:48.172907\n",
      "13 9960 9980\n",
      "After augmentation, size of train_set:  80  lake set:  9960  val set:  47\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.17422232031822205 1.0 0.3369501531124115 0.9574468085106383 6.984281040728092 0.7932692307692307 12.983327627182007\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  0.0 45.73\n",
      "val, test error% for class  1  :  9.09 5.64\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "18 / 20  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2022-01-04 03:12:48.539043\n",
      "18 9940 9960\n",
      "After augmentation, size of train_set:  100  lake set:  9940  val set:  65\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.05330041307024658 1.0 0.5412293672561646 0.9384615384615385 10.146788820624352 0.7980769230769231 14.081515550613403\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  6.45 41.03\n",
      "val, test error% for class  1  :  5.88 7.69\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "19 / 20  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2022-01-04 03:13:49.820186\n",
      "19 9920 9940\n",
      "After augmentation, size of train_set:  120  lake set:  9920  val set:  84\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.1607395652681589 0.9916666666666667 0.18032774329185486 0.9523809523809523 5.625427916646004 0.8060897435897436 13.229187488555908\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  5.0 30.34\n",
      "val, test error% for class  1  :  4.55 12.82\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "17 / 20  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2022-01-04 03:14:51.820931\n",
      "17 9900 9920\n",
      "After augmentation, size of train_set:  140  lake set:  9900  val set:  101\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.2612133597722277 0.9928571428571429 0.3285241425037384 0.9504950495049505 9.595471106469631 0.8108974358974359 19.88993787765503\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  6.38 43.59\n",
      "val, test error% for class  1  :  3.7 4.1\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "18 / 20  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2022-01-04 03:16:00.591664\n",
      "18 9880 9900\n",
      "After augmentation, size of train_set:  160  lake set:  9880  val set:  119\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.20837038522586226 0.99375 0.1821449249982834 0.9747899159663865 5.177553508430719 0.7852564102564102 15.66502070426941\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  5.17 50.43\n",
      "val, test error% for class  1  :  0.0 4.1\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "20 / 20  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2022-01-04 03:17:07.108915\n",
      "20 9860 9880\n",
      "After augmentation, size of train_set:  180  lake set:  9860  val set:  139\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.14906009915284812 1.0 0.06249784678220749 0.9784172661870504 5.683890074491501 0.8493589743589743 20.676458835601807\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  2.99 18.38\n",
      "val, test error% for class  1  :  1.39 13.08\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n",
      "#### SELECTION COMPLETE ####\n",
      "20 / 20  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2022-01-04 03:18:19.017357\n",
      "20 9840 9860\n",
      "After augmentation, size of train_set:  200  lake set:  9840  val set:  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.24476828845217824 0.995 0.08260950446128845 0.9811320754716981 5.048657655715942 0.8221153846153846 24.142332077026367\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  0.0 20.51\n",
      "val, test error% for class  1  :  3.66 16.15\n",
      "val set targets:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Queries Updated\n",
      "private set targets:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Model Updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 20 of 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SELECTION COMPLETE ####\n",
      "19 / 20  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2022-01-04 03:19:36.225977\n",
      "19 9820 9840\n",
      "After augmentation, size of train_set:  220  lake set:  9820  val set:  178\n",
      "Selection Epoch  9  Training epoch [ 4 ]  Training Acc:  0.7909090909090909\r"
     ]
    }
   ],
   "source": [
    "# LOGDETCMI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b70e37-9ab2-4973-bc85-e1a4389fdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLCMI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'flmic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981e6f2-2993-46c0-8896-b81a137712cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL2MI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'fl2mi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a57e1-ec17-4228-bc4b-41853580cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL1MI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'fl1mi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbe915-a18e-4183-b252-21e0f630bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BADGE\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"badge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd74f3-c9f6-402c-887c-9e0ed9ca84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"us\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8638f5-d71f-4251-beff-81616c3e9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLISTER\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"glister-tss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489760d-001e-4202-8111-46e03e998409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCMI+DIV\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'div-gcmi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b09dd-ce2e-4e87-9bd6-d69d343a0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCMI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'gcmi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12dcbc-712b-4935-9506-a49e699f2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGDETMI\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49537200-9e1d-4a46-8627-5cdebaa0ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'fl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39383a95-8a8e-417d-a3f3-c5cfb198df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGDET\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'logdet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c249a-1fcd-4475-8854-e685b77c9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"random\",'random')\n",
    "# CORESET\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"coreset\")\n",
    "# LEASTCONF\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"leastconf\")\n",
    "# MARGIN SAMPLING\n",
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"margin\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python375jvsc74a57bd0fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
